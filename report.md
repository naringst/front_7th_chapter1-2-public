# AI와 테스트를 활용한 안정적인 기능 개발 리포트

## 사용하는 도구를 선택한 이유가 있을까요? 각 도구의 특징에 대해 조사해본적이 있나요?

**Cursor를 선택한 이유:**

- 실제로는 회사에서 평소에 사용하고 있어서 익숙한 IDE
- 코드베이스 전체를 이해하고 컨텍스트를 유지할 수 있는 에이전트 기능
- 복잡한 리팩토링과 디버깅 작업에 강점
- 파일 간 관계를 이해하고 일관성 있게 수정 가능
- IDE로서 코드 수정에 편리함

**Orchestrator 워크플로우를 도입한 이유:**

- TDD 사이클을 체계적으로 자동화
- 8단계 워크플로우(분해 → 설계 → 테스트 → 구현)를 명확히 정의
- 각 단계의 검증(Validation)을 통해 품질 보장
- 실패 시 재시도 및 피드백 메커니즘 내장

**BMAD-method 조사:**

- 처음으로 알게 된 개념을 찾아보았습니다
- 각 에이전트에게 페르소나를 부여하는 방식
- 체크리스트와 docs를 세분화하여 관리하는 방법
- 이 개념을 차용하여 개발을 진행했습니다

**특징 조사:**

- 각 도구의 명령어(command)를 `.cursor/commands/` 디렉토리에 구조화하여 저장
- BMAD-method를 참조하여 역할 기반 에이전트 분리
- 각 에이전트의 역할과 책임 범위를 명확히 정의
- "되묻기 규칙"과 human-in-the-loop으로 진행 과정 제어

**에이전트 구성 (최종 설계):**

프로덕션 사용을 목표로 했으며, 작은 단위와 명확한 입출력이 핵심이었습니다.

1. **split-by-number**: 번호대로 기능을 나누는 에이전트
2. **feature-decomposer**: 기능을 Epic → Story → Flow로 세분화
3. **test-designer**: 통합 테스트 디자인
4. **integration-test-writer**: 통합 테스트 작성
5. **integration-test-evaluator**: 통합 테스트 체크리스트 기반 평가 (90점 이상만 사용)
6. **unit-candidate-finder**: 단위테스트 후보 식별
7. **unit-test-writer**: 단위 테스트 작성
8. **developer**: 테스트 기반 기능 개발
9. **debug-doctor**: 에러 발생 시 원인 분석 및 수정 전문화
10. **refactor**: 코드 리팩토링

**체계적 구조:**

```
.cursor/
├── commands/          # 역할별 에이전트 정의
├── outputs/           # 각 단계의 출력물 저장
│   ├── 2-splited-features/
│   ├── 3-integration-test-design/
│   └── 4-integration-to-unit/
└── checklists/        # 검증 체크리스트
    ├── breakdown-checklist.md
    ├── how-to-design-test.md
    ├── how-to-test.md
    ├── kent-beck-test.md
    └── integration-test-quality.md
```

**발전 과정:**

- **1차 시도**: 역할 기반(PO, PM, SM) → PM이 명세를 임의 추가하여 실패
- **2차 시도**: human-in-the-loop + 되묻기 규칙 → 역할 혼재 문제
- **최종 설계**: 도구 기반 명확한 분리 → 각 에이전트 역할 명확화

## 테스트를 기반으로 하는 AI를 통한 기능 개발과 없을 때의 기능개발은 차이가 있었나요?

**테스트 기반 개발(TDD)의 핵심 차이점:**

1. **명확한 목표 지향성**

   - 테스트가 실패하는 상태부터 시작 → 무엇을 구현해야 하는지 명확
   - "이 테스트를 통과시키면 기능이 완성"이라는 명확한 완료 기준

2. **디버깅 효율성 극대화**

   - 실패한 테스트 → 에러 메시지 → 원인 파악 → 수정 → 테스트 통과
   - AI에게 "이 테스트가 실패하는 이유를 찾아줘"라고 요청하면 구체적인 원인 제시

3. **리그레션 방지**

   - 기능 추가 시 기존 테스트로 회귀 확인 자동화
   - "모든 테스트가 통과하는가?"가 품질 기준

4. **단계별 검증 가능**
   - 통합 테스트(240개)가 모두 통과 = 전체 기능 정상 작동
   - 단위 테스트로 세부 로직 검증 가능

**AI를 통한 기능 개발의 특별함:**

- AI를 통한 기능 개발은 테스트 코드를 작성하는 문법에 대한 이해보다는, "어떤 동작을 검증해야 하는지"에 집중할 수 있게 해줍니다
- 테스트가 명세서 역할을 하며, AI가 이 명세를 바탕으로 구현 코드를 작성합니다
- 수동 테스트 없이도 신뢰할 수 있는 코드를 얻을 수 있습니다

**없을 때의 문제점:**

- 구현 후 수동 테스트 → 놓친 엣지 케이스 발생
- 리팩토링 시 기존 기능 깨짐 위험
- AI가 생성한 코드의 정확성 검증 어려움

## AI의 응답을 개선하기 위해 추가했던 여러 정보(context)는 무엇인가요?

1. **구조화된 PRD 문서**

   - FEATURE 문서를 Epic → Story → Flow로 분해
   - 각 Flow에 대응하는 테스트 케이스 설계 문서 생성

2. **기존 코드베이스 맥락**

   - 타입 정의, 유틸리티 함수, 기존 패턴
   - 예: `Event`, `EventForm` 타입, `findRepeatGroup`, `applyEventUpdate` 등

3. **테스트 디자인 문서**

   - `.cursor/outputs/3-integration-test-design/feature{n}-test-design.md`
   - 각 테스트 케이스의 입력/기대 결과 명시

4. **실패한 테스트의 구체적 에러 로그**

   - "이 테스트가 실패합니다: `expected 5 but got 0`"
   - 스택 트레이스, 타임아웃 정보 등 포함

5. **현재 상태 정보**

   - "현재 6/10 테스트 통과, 4개 실패"
   - 실패한 테스트의 공통 패턴 제시

6. **워크플로우 상태**

   - 현재 어떤 Stage에 있는지 (예: Stage 8/8 - Integration TDD)
   - 이전 단계의 아티팩트 경로

7. **체크리스트 기반 검증 기준**
   - breakdown-checklist.md: 12개 항목으로 기능 분해 품질 평가
   - integration-test-quality.md: 통합 테스트 90점 기준 명시
   - 각 에이전트에게 해당 체크리스트를 컨텍스트로 제공

## 이 context를 잘 활용하게 하기 위해 했던 노력이 있나요?

1. **체계적인 디렉토리 구조**

   ```
   .cursor/
   ├── commands/          # 역할별 에이전트 정의
   ├── outputs/           # 각 단계의 출력물 저장
   │   ├── 2-splited-features/
   │   ├── 3-integration-test-design/
   │   └── 4-integration-to-unit/
   └── checklists/        # 검증 체크리스트
   ```

2. **명시적인 참조(@ 파일, 파일 경로)**

   - `@FEATURE3.md`, `@feature3-test-design.md` 등으로 정확한 문서 참조
   - 파일 경로를 명시하여 컨텍스트 범위 명확화
   - 앞선 에이전트의 출력물을 뒷 에이전트가 참고하도록 outputs에 저장

3. **단계별 검증 메커니즘**

   - 각 Stage마다 Validation 단계 필수
   - 실패 시 이전 단계로 롤백하거나 재시도
   - integration-test-evaluator로 통합 테스트 90점 이상만 사용 가능

4. **에러 발생 시 전체 맥락 제공**

   - 실패한 테스트 코드 전체
   - 관련 구현 코드
   - 에러 메시지와 스택 트레이스
   - 예: "이 테스트가 실패합니다: ... /debug-doctor /developer"

5. **역할 기반 에이전트 활용**

   - `/debug-doctor`: 원인 분석 전문화 (가설 검증 워크플로우)
   - `/developer`: 구현 전문화
   - `/refactor`: 리팩토링 전문화
   - 각 에이전트에게 맞는 컨텍스트만 제공

6. **체크리스트 기반 검증 체계**
   - `breakdown-checklist.md`: 기능 분해 품질 보장
   - `how-to-design-test.md`: 테스트 설계 가이드라인
   - `how-to-test.md`: 테스트 작성 베스트 프랙티스
   - `integration-test-quality.md`: 통합 테스트 90점 기준
   - 각 단계마다 체크리스트로 자동 평가
   - 참고자료: how-to-test.md (배휘동님 github), kent-beck-test.md

## 생성된 여러 결과는 만족스러웠나요? AI의 응답을 어떤 기준을 갖고 '평가(evaluation)'했나요?

**평가 기준 (우선순위 순):**

1. **테스트 통과율** (최우선)

   - 모든 통합 테스트 240개 통과 = 기능 완성
   - 단위 테스트 통과 = 세부 로직 정확

2. **코드 품질**

   - Linter 에러 0개
   - TypeScript 타입 안전성
   - 중복 코드 최소화

3. **기능 정확성**

   - 실제 앱에서 동작 확인
   - 엣지 케이스 처리 (예: 윤년, 31일 처리)

4. **테스트 안정성**
   - 일시적 실패(flaky test) 없음
   - 타임아웃 설정 적절
   - 비동기 처리 올바름

**평가 방법:**

- **체크리스트 기반 평가**: 통합 테스트는 90점 이상만 승인, 아니면 재작성
- **자동 검증**: 각 단계마다 체크리스트를 통과해야 다음 단계로 진행
- **점수 기준**: integration-test-evaluator가 체크리스트 기반으로 점수를 매기고, 90점 이하면 test-writer가 재작성하도록 설정

**만족도:**

- 초기: 60% (테스트 불안정, 디버깅 반복)
- 최종: 95% (모든 테스트 통과, 안정적)

**개선 과정:**

- 테스트 안정화에 상당한 시간 투자 (약 30% 시간)
- 하지만 결과적으로 리그레션 방지 및 신뢰성 확보

## AI에게 어떻게 질문하는것이 더 나은 결과를 얻을 수 있었나요? 시도했던 여러 경험을 알려주세요.

**효과적인 질문 패턴:**

1. **구체적인 에러 정보 제공**

   ```
   ❌ "테스트가 안 돼"
   ✅ "TC-3-1-3이 실패합니다: expected 5 but got 0.
       postedEvents 배열이 비어있습니다.
       겹침 다이얼로그가 나타나서 저장이 막히는 것 같습니다."
   ```

2. **현재 상태 + 기대 결과 명시**

   ```
   "현재 6/10 테스트 통과.
   TC-4-1-1, TC-4-1-3이 실패합니다.
   원인: time validation 에러.
   목표: 모든 테스트 통과"
   ```

3. **관련 파일/컨텍스트 명시**

   ```
   "@feature4-integration.spec.tsx
   @useEventOperations.ts
   이 두 파일을 보면서 문제를 찾아줘"
   ```

4. **역할 기반 질문**

   ```
   "/debug-doctor 근본 원인을 파악하고 수정하세요"
   "/developer 테스트를 통과시키는 최소 구현을 작성하세요"
   ```

5. **작업 범위 명확화**

   ```
   "FEATURE3 남은 케이스 먼저 다 수정해"
   "우선 여기까지 커밋하고, 나머지 안되는 부분 있는지 확인해보자"
   ```

6. **반복적인 피드백 제공**
   ```
   "1. 반복 일정 수정 기능 자체가 안돼 => 그래서 다 안되는거네"
   "그러면 수정로직 가서 수정 로직에서 원인을 찾아봐"
   ```

**효과:**

- 초기: 질문 1회 → 부분 해결 → 재질문 반복 (평균 3-4회)
- 후기: 명확한 질문 → 전체 해결 (평균 1-2회)

## AI에게 지시하는 작업의 범위를 어떻게 잡았나요? 범위를 좁게, 넓게 해보고 결과를 적어주세요. 그리고 내가 생각하는 적절한 단위를 말해보세요.

**범위 실험 결과:**

1. **너무 넓은 범위** (실패 사례)

   ```
   ❌ "FEATURE3 전체 구현해줘"
   - 결과: AI가 모든 테스트를 한 번에 작성하려 함
   - 문제: 복잡한 상호 의존성, 부분 실패 시 롤백 어려움
   - 시간: 예상 2시간 → 실제 6시간
   ```

2. **너무 좁은 범위** (비효율적)

   ```
   ❌ "이 한 줄 수정해줘"
   - 결과: 전체 맥락 이해 못함
   - 문제: 근본 원인 해결 못하고 증상만 수정
   - 시간: 반복 작업으로 누적 시간 증가
   ```

3. **적절한 범위** (성공 사례)

   ```
   ✅ "FEATURE3 테스트 안정화 - TC-3-1-3이 실패하는데
       postedEvents가 비어있어. 겹침 다이얼로그 문제일 것 같아"
   - 결과: 관련된 몇 개 테스트만 함께 수정
   - 효과: 근본 원인(초기 이벤트 겹침) 해결
   - 시간: 예상 30분 → 실제 20분
   ```

4. **Story 단위** (가장 효과적)
   ```
   ✅ "Story 1: 반복 종료 조건 설정 관련 테스트 모두 통과시켜줘"
   - 결과: 하나의 Story에 속한 4개 테스트를 함께 수정
   - 효과: Story 단위 완성도 높음
   - 시간: 예상 1시간 → 실제 45분
   ```

**적절한 단위:**

- **Story 단위** (약 3-5개 테스트 케이스)
  - 하나의 사용자 시나리오 완성
  - 관련된 파일 그룹을 함께 수정
  - 완료 기준 명확 (Story의 모든 TC 통과)
- **실험 결과**: 범위를 좁게 잡는 것이 가장 효과적이었습니다
  - 너무 넓으면: 복잡한 상호 의존성으로 부분 실패 시 롤백 어려움
  - 너무 좁으면: 전체 맥락 이해 못하고 증상만 수정
  - **좁게**: Story 단위로 점진적 개발이 가장 안정적

**범위 결정 원칙:**

1. **의존성 기준**: 함께 변경해야 하는 파일들을 그룹으로
2. **실패 패턴 기준**: 같은 원인으로 실패하는 테스트들을 함께
3. **완성도 기준**: 하나의 기능 단위가 완전히 동작하는 수준
4. **좁게 시작**: 작은 단위부터 확실히 완성하고, 점진적으로 확장

## 동기들에게 공유하고 싶은 좋은 참고자료나 문구가 있었나요? 마음껏 자랑해주세요.

**1. Orchestrator 워크플로우 설계**

```
"8단계 TDD 워크플로우로 기능 개발을 체계화했습니다.
Breakdown → Design → Test → Implement → Refactor
각 단계의 검증을 통해 품질을 보장합니다."
```

**2. 테스트 안정화를 위한 베스트 프랙티스**

```typescript
// ❌ 비동기 처리 없음
await userEvent.type(input, '2025-10-05');
expect(postedEvents).toHaveLength(5);

// ✅ waitFor로 안정적 검증
await userEvent.clear(input);
await userEvent.type(input, '2025-10-05');
await waitFor(() => expect(input.value).toBe('2025-10-05'));
await waitFor(() => expect(postedEvents).toHaveLength(5), { timeout: 10000 });
```

**3. 역할 기반 에이전트 활용**

```
"/debug-doctor: 원인 분석 전문화
/developer: 구현 전문화
/refactor: 리팩토링 전문화
각 역할에 맞는 질문을 하면 더 정확한 답변을 얻을 수 있습니다."
```

**4. 컨텍스트 제공의 중요성**

```
"AI에게 질문할 때:
1. 구체적인 에러 정보 제공
2. 관련 파일 명시 (@파일경로)
3. 현재 상태 + 기대 결과 명시
4. 작업 범위 명확화

이렇게 하면 질문 횟수가 3-4회에서 1-2회로 줄어듭니다."
```

**5. TDD의 강점**

```
"테스트가 실패하는 상태부터 시작하면:
- 무엇을 구현해야 하는지 명확
- 언제 완료되었는지 알 수 있음
- 리팩토링이 안전함

240개 테스트가 모두 통과 = 모든 기능이 정상 작동"
```

## AI가 잘하는 것과 못하는 것에 대해 고민한 적이 있나요? 내가 생각하는 지점에 대해 작성해주세요.

**AI가 잘하는 것:**

1. **패턴 기반 수정**

   - 유사한 코드 패턴을 찾아 일괄 수정
   - 예: 모든 테스트에 `waitFor` 추가

2. **구조적 변경**

   - 파일 구조 재구성
   - 함수 추출/병합
   - 타입 정의 추가

3. **에러 원인 분석**

   - 스택 트레이스 분석
   - 로그 기반 디버깅
   - 코드 플로우 추적

4. **테스트 코드 생성**
   - 설계 문서 기반 테스트 작성
   - 유사 테스트 패턴 재사용

**AI가 못하는 것 (한계):**

1. **비즈니스 로직 판단**

   - "이게 올바른 동작인가?" 판단 어려움
   - 예: "종료 날짜 미입력 시 기본값 적용" - 이게 맞는지 AI는 모름

2. **타이밍 이슈 이해**

   - 비동기 상태 업데이트 타이밍 문제
   - 테스트 안정화를 위한 적절한 `waitFor` 위치 판단
   - **해결**: 구체적인 타임아웃 정보 제공 필요

3. **전체 맥락 이해의 한계**

   - 하나의 파일만 보면 다른 파일 영향 고려 못함
   - 예: `App.tsx` 수정이 `useEventOperations.ts`에 영향
   - **해결**: 관련 파일을 함께 제공

4. **점진적 개선 방향 판단**

   - 여러 문제 중 어떤 것부터 해결해야 할지 판단 어려움
   - **해결**: 우선순위를 명시적으로 제시

5. **실제 사용자 경험 판단**
   - "이 UI/UX가 좋은가?" 판단 어려움
   - 테스트로 검증 가능한 것만 판단

**대응 전략:**

- AI가 잘하는 것: 패턴 기반 작업 위임
- AI가 못하는 것: 사람이 판단하고 구체적 지시

## 마지막으로 느낀점에 대해 적어주세요!

**1. 테스트가 있으면 AI 개발이 완전히 달라집니다**

- 테스트 = 명확한 완료 기준
- 실패한 테스트 = 구체적인 수정 목표
- 모든 테스트 통과 = 기능 완성 확신

**2. 체계적인 워크플로우가 필수입니다**

- Orchestrator처럼 단계를 나누고 검증하는 과정이 없으면
- AI가 만들어낸 코드의 품질을 신뢰할 수 없음
- 각 단계의 아티팩트(문서, 테스트)가 다음 단계의 입력이 되면서 일관성 확보

**3. 디버깅 시간이 가장 많았습니다**

- 초기 구현: 30% 시간
- 테스트 안정화: 30% 시간
- 디버깅: 40% 시간

하지만 이 투자가 결국 신뢰성을 보장했습니다.

**4. AI에게 구체적인 정보를 주는 것이 핵심입니다**

- 에러 메시지, 관련 파일, 현재 상태
- 이 세 가지만 명확히 하면 AI의 답변 품질이 급상승

**5. "완벽하게 작동하는 코드"를 얻으려면 시간이 필요합니다**

- 초기 생성 코드: 60% 정도만 작동
- 디버깅 및 안정화: +40% 시간 투자
- 하지만 최종적으로는 **240개 테스트가 모두 통과하는 안정적인 코드** 완성

**6. TDD + AI = 강력한 조합**

- AI가 테스트 코드를 생성하고
- 테스트가 실패하는 것을 보면서
- AI가 구현을 수정하고
- 테스트가 통과할 때까지 반복
- 이 사이클이 체계화되면 개발 속도와 품질 모두 확보

**7. 실무 적용 가능성**

- 이번에 구축한 에이전트 워크플로우를 실무에서 활용할 수 있을지 고민했습니다
- 실무에서 반복적인 기능 개발에 대해서는 에이전트로 개발 효율성을 높일 수 있을 것 같습니다
- 하지만 모든 기능을 에이전트에 맡기기보다는, 비즈니스 로직 판단이 필요한 부분은 사람이 직접 하고, 패턴이 반복되는 부분은 에이전트에게 위임하는 것이 좋겠습니다

**결론:**
AI는 도구입니다. 테스트와 워크플로우라는 "틀"이 있어야 그 도구를 효과적으로 활용할 수 있습니다. 이번 프로젝트를 통해 그 틀을 만드는 것이 얼마나 중요한지 깨달았습니다. 특히 실무에서 에이전트를 활용해 개발 효율성을 높이고 싶다면, 체계적인 워크플로우와 명확한 검증 기준이 필수적입니다.
